from qwen2.modeling_qwen2 import Qwen2ForCausalLM
from qwen2.configuration_qwen2 import Qwen2Config
from modelscope import AutoTokenizer
import json
import pandas as pd
import torch
import torch.nn as nn
from datasets import Dataset
from swanlab.integration.huggingface import SwanLabCallback
import os
import swanlab
from peft import LoraConfig, TaskType, get_peft_model
from datasets import load_dataset
from safetensors.torch import load_file
from tqdm import tqdm
from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
config = Qwen2Config.from_json_file("./qwen/Qwen2___5-1___5B-Instruct/config.json")
model = Qwen2ForCausalLM(config)
model_weights = load_file("./qwen/Qwen2___5-1___5B-Instruct/model.safetensors")
model.load_state_dict(model_weights, strict=False)
model = model.to(dtype=torch.bfloat16, device="cuda")

# åŠ è½½tokenizer
tokenizer = AutoTokenizer.from_pretrained("./qwen/Qwen2___5-1___5B-Instruct/", use_fast=False, trust_remote_code=True)

# æ‰“å°æ‰€æœ‰å¯è®­ç»ƒå‚æ•°ä»¥åŠå¯è®­ç»ƒå‚æ•°é‡
def print_trainable_parameters(model):
    total_params = 0
    total_trainable_params = 0
    for name, param in model.named_parameters():
        param_count = param.numel()  # è®¡ç®—å‚æ•°æ•°é‡
        total_params += param_count
        if 'adapter' in name:
            param.requires_grad_(True)
        else:
            param.requires_grad_(False)
        model.lm_head.weight.requires_grad = True
        if param.requires_grad:
            total_trainable_params += param_count
            print(f"{name}: {param_count} trainable")
    print(f"Total parameters: {total_params}")
    print(f"Trainable parameters: {total_trainable_params}")
    return total_trainable_params

# æŒ‡å®šadapterå¾®è°ƒ
print_trainable_parameters(model)
print(model.model.forward.__code__.co_varnames) # å‰å‘ä¼ æ’­å‚æ•°

# æ•°æ®å¤„ç†å‡½æ•°
def process_func(example):
    MAX_LENGTH = 512
    
    input_ids = tokenizer.apply_chat_template(
        example["messages"],
        tokenize=True,
        add_generation_prompt=False,
        max_length=MAX_LENGTH,
        truncation=True
    )
    
    labels = input_ids.copy()
    user_len = len(
        tokenizer.apply_chat_template(
            example["messages"][:2],
            tokenize=True,
            add_generation_prompt=False,
            max_length=MAX_LENGTH,
            truncation=True
        )
    )

    labels[:user_len] = [-100] * user_len
    
    return {
        "input_ids": input_ids,
        "labels": labels,
        "attention_mask": [1] * len(input_ids)
    }

# åŠ è½½è®­ç»ƒå’ŒéªŒè¯æ•°æ®
TRAIN_FILE = "./train_reasoning.jsonl"
VAL_FILE = "./val_reasoning.jsonl"
print("æ­£åœ¨åŠ è½½å¹¶å¤„ç†æ•°æ®é›†...")
dataset = load_dataset("json", data_files={"train": TRAIN_FILE, "validation": VAL_FILE})
column_names = dataset["train"].column_names
tokenized_dataset = dataset.map(
    process_func,
    batched=False,
    remove_columns=column_names,
    desc="Running tokenizer"
)

# è®­ç»ƒå‚æ•°
output_dir="./output/Qwen2.5_adapter_10"
args = TrainingArguments(
    output_dir="./output/Qwen2.5_adapter_10",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    logging_steps=10,
    num_train_epochs=3,
    save_strategy="steps",  # Enable saving at regular steps
    save_steps=1020,  # Save every 100 steps
    learning_rate=1e-4,
    save_on_each_node=True,  # Ensure saving on each node in multi-node training
    gradient_checkpointing=True,
    report_to="none",
)


swanlab_callback = SwanLabCallback(
    project="Qwen2.5-fintune",
    experiment_name="Qwen2.5-1.5B-Adapter-10",
    description="ä½¿ç”¨é€šä¹‰åƒé—®Qwen2-1.5B-Instructæ¨¡å‹åœ¨æ•°æ®é›†ä¸Šå¾®è°ƒã€‚",
    config={
        "model": "qwen/Qwen2.5-1.5B-Instruct",
        "dataset": "./train.jsonl",
    }
)


trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),
    callbacks=[swanlab_callback]
)

trainer.train()


# éªŒè¯é›†è¯„ä¼°
RESULT_FILE = "result.jsonl"
MAX_NEW_TOKENS = 512
model.eval() # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼

print(f"æ­£åœ¨è¯»å–éªŒè¯é›†: {VAL_FILE}")
data_samples = []
with open(VAL_FILE, "r", encoding="utf-8") as f:
    for line in f:
        if line.strip():
            data_samples.append(json.loads(line))

print(f"å…±åŠ è½½ {len(data_samples)} æ¡éªŒè¯æ•°æ®ã€‚")
def predict(messages, model, tokenizer):
    """
    ç”Ÿæˆå›å¤å¹¶åªè¿”å›ç”Ÿæˆçš„æ–‡æœ¬éƒ¨åˆ†
    """
    # æ„é€  Prompt
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    inputs = tokenizer([text], return_tensors="pt").to(model.device)
   
    with torch.no_grad():
        generated_ids = model.generate(
            inputs.input_ids,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=False,
            temperature=0.1,  # éªŒè¯æ—¶æ¸©åº¦è®¾ä½ä¸€ç‚¹ï¼Œä¿è¯ç»“æœç¡®å®šæ€§
            top_p=0.9
        )

    # è£å‰ªæ‰ Input éƒ¨åˆ†ï¼Œåªå– Output
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)
    ]
 
    
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return response

def check_correctness(response, ground_truth):
    """
    åˆ¤åˆ«é€»è¾‘ï¼šåªè¦ç”Ÿæˆçš„å›ç­”åŒ…å«å®Œæ•´çš„ ground truthï¼Œå³ä¸ºæ­£ç¡®
    """
    # ç®€å•çš„å­—ç¬¦ä¸²åŒ…å«åˆ¤æ–­
    return ground_truth in response


base_correct_count = 0
ft_correct_count = 0
total_count = 0
results_log = []

print("\nğŸš€ å¼€å§‹è‡ªåŠ¨åŒ–è¯„ä¼°...")
pbar = tqdm(data_samples, desc="Evaluating", unit="sample")

for sample in pbar:
    total_count += 1
    
    # æå–è¾“å…¥å’Œæ ‡å‡†ç­”æ¡ˆ
    input_messages = sample["messages"][:-1]
    ground_truth = sample["messages"][-1]["content"]
    ft_response = predict(input_messages, model, tokenizer)
    is_ft_correct = check_correctness(ft_response, ground_truth)
    if is_ft_correct:
        ft_correct_count += 1

    results_log.append({
        "input": input_messages[-1]["content"], 
        "ground_truth": ground_truth,
        "ft_response": ft_response,
        "ft_correct": is_ft_correct
    })

    current_ft_acc = ft_correct_count / total_count
    pbar.set_postfix({ 
        "FT_Acc": f"{current_ft_acc:.2%}"
    })

final_ft_acc = ft_correct_count / total_count

print("\n" + "="*50)
print("æœ€ç»ˆè¯„ä¼°æŠ¥å‘Š")
print("="*50)
print(f"éªŒè¯é›†æ€»æ•°: {total_count}")
print("åˆ¤åˆ«æ ‡å‡†: ç”Ÿæˆå†…å®¹å¿…é¡»åŒ…å« Ground Truth")
print("-" * 30)
print(f"å¾®è°ƒæ¨¡å‹ æ­£ç¡®æ•°: {ft_correct_count}")
print(f"å¾®è°ƒæ¨¡å‹ å‡†ç¡®ç‡: {final_ft_acc:.2%}")
print("="*50)

# ä¿å­˜è¯¦ç»†ç»“æœ
with open(RESULT_FILE, "w", encoding="utf-8") as f:
    json.dump(results_log, f, ensure_ascii=False, indent=2)
print(f"è¯¦ç»†å¯¹æ¯”æ—¥å¿—å·²ä¿å­˜è‡³: {RESULT_FILE}")


swanlab.finish()
