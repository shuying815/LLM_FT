import json
import pandas as pd
import torch
from datasets import Dataset
from modelscope import snapshot_download, AutoTokenizer
from swanlab.integration.huggingface import SwanLabCallback
from peft import LoraConfig, TaskType, get_peft_model
from transformers import AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq
import os
import swanlab
from datasets import load_dataset
from trl import DPOTrainer, DPOConfig
from transformers import DataCollatorWithPadding, Trainer
from tqdm import tqdm
import torch.nn.functional as F

class SFTWithDPOTrainer(Trainer):
    def __init__(self, dpo_beta=0.1, dpo_lambda=0.05, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.dpo_beta = dpo_beta
        self.dpo_lambda = dpo_lambda

    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        """
        inputs:
        {
          input_ids, attention_mask, labels,
          prompt, chosen, rejected
        }
        """

        # ---------
        # 1. SFT lossï¼ˆä¸»æŸå¤±ï¼‰
        # ---------
        outputs = model(
            input_ids=inputs["input_ids"],
            attention_mask=inputs["attention_mask"],
            labels=inputs["labels"]
        )
        sft_loss = outputs.loss
        # ---------
        # 2. DPO lossï¼ˆæ­£åˆ™é¡¹ï¼‰
        # ---------
        with torch.no_grad():
            prompt_ids = inputs["prompt_ids"]
            chosen_ids = inputs["chosen_ids"]
            rejected_ids = inputs["rejected_ids"]

        # æ‹¼ prompt + answer
        def get_logp(answer_ids):
            full_ids = torch.cat([prompt_ids, answer_ids], dim=1)
            attn = torch.ones_like(full_ids)

            out = model(full_ids, attention_mask=attn)
            logits = out.logits[:, :-1, :]
            labels = full_ids[:, 1:]

            log_probs = F.log_softmax(logits, dim=-1)
            token_logp = torch.gather(
                log_probs,
                dim=-1,
                index=labels.unsqueeze(-1)
            ).squeeze(-1)

            # åªç®— answer éƒ¨åˆ†
            return token_logp[:, -answer_ids.size(1):].sum(dim=1)

        logp_chosen = get_logp(chosen_ids)
        logp_rejected = get_logp(rejected_ids)

        dpo_loss = -torch.mean(
            F.logsigmoid(self.dpo_beta * (logp_chosen - logp_rejected))
        )

        # ---------
        # 3. æ€»æŸå¤±
        # ---------
        loss = sft_loss + self.dpo_lambda * dpo_loss

        return (loss, outputs) if return_outputs else loss



def print_trainable_parameters(model):
    total_params = 0
    total_trainable_params = 0
    for name, param in model.named_parameters():
        param_count = param.numel()  # è®¡ç®—å‚æ•°æ•°é‡
        total_params += param_count
        if param.requires_grad:
            total_trainable_params += param_count
            print(f"{name}: {param_count} trainable")
    print(f"Total parameters: {total_params}")
    print(f"Trainable parameters: {total_trainable_params}")
    return total_trainable_params
    
# åœ¨modelscopeä¸Šä¸‹è½½Qwenæ¨¡å‹åˆ°æœ¬åœ°ç›®å½•ä¸‹
#model_dir = snapshot_download("qwen/Qwen2.5-1.5B-Instruct", cache_dir="./", revision="master")

# TransformersåŠ è½½æ¨¡å‹æƒé‡
tokenizer = AutoTokenizer.from_pretrained("../qwen/qwen/Qwen2___5-1___5B-Instruct/", use_fast=False, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("../qwen/qwen/Qwen2___5-1___5B-Instruct/", device_map="auto", torch_dtype=torch.bfloat16)
model.enable_input_require_grads()  # å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹æ—¶ï¼Œè¦æ‰§è¡Œè¯¥æ–¹æ³•

if tokenizer.bos_token is None:   # qwenæ²¡æœ‰bos_tokenï¼Œè¦è®¾ç½®ä¸€ä¸‹ï¼Œä¸ç„¶dpo trainæ—¶ä¼šæŠ¥é”™ã€‚
    tokenizer.add_special_tokens({"bos_token": tokenizer.eos_token})
    tokenizer.bos_token_id = tokenizer.eos_token_id
tokenizer.padding_side = "right"

import random
from datasets import Dataset

# -------------------------
# 1. è¯»å–å…¨ç±»åˆ«
# -------------------------
def get_all_classes(path):
    classes = set()
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            item = json.loads(line)
            for msg in item["messages"]:
                if msg["role"] == "assistant":
                    classes.add(msg["content"].strip())
                    break
    return sorted(list(classes))


# -------------------------
# 2. æå– prompt / label
# -------------------------
def extract_label(item):
    for msg in item["messages"]:
        if msg["role"] == "assistant":
            return msg["content"].strip()
    return None

def extract_prompt(item):
    for msg in item["messages"]:
        if msg["role"] == "user":
            return msg["content"].strip()
    return ""


# -------------------------
# 3. æ„å»º DPO æ•°æ®
# -------------------------

from sentence_transformers import SentenceTransformer
import numpy as np

embed_model = SentenceTransformer("./paraphrase-multilingual-MiniLM-L12-v2")

from functools import partial

def build_class_embeddings(all_classes):
    texts = [f"å­¦ç§‘é¢†åŸŸï¼š{c}" for c in all_classes]
    embs = embed_model.encode(texts, normalize_embeddings=True)
    return {
        c: embs[i] for i, c in enumerate(all_classes)
    }

TRAIN_FILE = "./train.jsonl"
VAL_FILE = "./val.jsonl"
print("æ­£åœ¨åŠ è½½å¹¶å¤„ç†æ•°æ®é›†...")
dataset = load_dataset("json", data_files={"train": TRAIN_FILE, "validation": VAL_FILE})
all_classes = get_all_classes(TRAIN_FILE)
CLASS_EMB = build_class_embeddings(all_classes)

def select_semantic_rejected(label, all_classes, class_emb, confusable_map=None):
    label_emb = class_emb[label]

    # 1ï¸âƒ£ å€™é€‰é›†åˆ
    if confusable_map and label in confusable_map:
        candidates = confusable_map[label]
    else:
        candidates = [c for c in all_classes if c != label]

    # 2ï¸âƒ£ è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    sims = {}
    for c in candidates:
        emb = class_emb[c]
        sims[c] = float(np.dot(label_emb, emb))  # å·² normalize

    # 3ï¸âƒ£ é€‰æœ€ç›¸ä¼¼çš„
    rejected = max(sims, key=sims.get)
    return rejected

def process_func_with_dpo(example, all_classes):
    MAX_LENGTH = 512

    # ---------
    # 1. SFT éƒ¨åˆ†
    # ---------
    messages = example["messages"]
    label = extract_label(example)

    input_ids = tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=False,
        max_length=MAX_LENGTH,
        truncation=True
    )

    labels = input_ids.copy()

    user_len = len(
        tokenizer.apply_chat_template(
            messages[:2],
            tokenize=True,
            add_generation_prompt=False,
            max_length=MAX_LENGTH,
            truncation=True
        )
    )

    labels[:user_len] = [-100] * user_len

    # ---------
    # 2. DPO éƒ¨åˆ†
    # ---------
   # negatives = [c for c in all_classes if c != label]
   # neg = random.choice(negatives)
    
    neg = select_semantic_rejected(label=label, all_classes=all_classes, class_emb=CLASS_EMB)


    # promptï¼ˆsystem + userï¼‰
    prompt_text = tokenizer.apply_chat_template(
        messages[:2],
        tokenize=False,
        add_generation_prompt=True
    )

    chosen_text = label
    rejected_text = neg

    prompt_ids = tokenizer(
        prompt_text,
        truncation=True,
        max_length=MAX_LENGTH,
        return_tensors=None
    )["input_ids"]

    chosen_ids = tokenizer(
        chosen_text,
        truncation=True,
        max_length=64,
        return_tensors=None
    )["input_ids"]

    rejected_ids = tokenizer(
        rejected_text,
        truncation=True,
        max_length=64,
        return_tensors=None
    )["input_ids"]

    return {
        # ---- SFT ----
        "input_ids": input_ids,
        "labels": labels,
        "attention_mask": [1] * len(input_ids),

        # ---- DPO ----
        "prompt_ids": prompt_ids,
        "chosen_ids": chosen_ids,
        "rejected_ids": rejected_ids,
    }

process_fn = partial(process_func_with_dpo, all_classes=all_classes)
tokenized_dataset = dataset.map(
    process_fn,
    remove_columns=dataset["train"].column_names,
    desc="Tokenizing with SFT + DPO"
)

config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    inference_mode=False,  # è®­ç»ƒæ¨¡å¼
    r=8,  # Lora ç§©
    lora_alpha=32,  # Lora alaphï¼Œå…·ä½“ä½œç”¨å‚è§ Lora åŸç†
    lora_dropout=0.1,  # Dropout æ¯”ä¾‹
)

model = get_peft_model(model, config)
print_trainable_parameters(model)  

training_args = TrainingArguments(
    output_dir="./dpo_optimize",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    learning_rate=1e-4,
    num_train_epochs=3,
    logging_steps=10,
    save_steps=500,
    bf16=True,
    remove_unused_columns=False,
    report_to="none",
)



swanlab_callback = SwanLabCallback(
    project="Qwen2.5-fintune",
    experiment_name="Qwen2.5-1.5B-dpo-fintune",
    description="ä½¿ç”¨é€šä¹‰åƒé—®Qwen2-1.5B-Instructæ¨¡å‹åœ¨æ•°æ®é›†ä¸Šå¾®è°ƒã€‚",
    config={
        "model": "qwen/Qwen2.5-1.5B-Instruct",
        "dataset": "./train.jsonl",
    }
)

from torch.nn.utils.rnn import pad_sequence
import torch

def sft_dpo_data_collator(features):
    # ---------
    # SFT éƒ¨åˆ†
    # ---------
    input_ids = [torch.tensor(f["input_ids"], dtype=torch.long) for f in features]
    labels = [torch.tensor(f["labels"], dtype=torch.long) for f in features]
    attention_mask = [torch.tensor(f["attention_mask"], dtype=torch.long) for f in features]

    input_ids = pad_sequence(
        input_ids, batch_first=True, padding_value=tokenizer.pad_token_id
    )
    labels = pad_sequence(
        labels, batch_first=True, padding_value=-100
    )
    attention_mask = pad_sequence(
        attention_mask, batch_first=True, padding_value=0
    )

    # ---------
    # DPO éƒ¨åˆ†
    # ---------
    prompt_ids = [torch.tensor(f["prompt_ids"], dtype=torch.long) for f in features]
    chosen_ids = [torch.tensor(f["chosen_ids"], dtype=torch.long) for f in features]
    rejected_ids = [torch.tensor(f["rejected_ids"], dtype=torch.long) for f in features]

    prompt_ids = pad_sequence(
        prompt_ids, batch_first=True, padding_value=tokenizer.pad_token_id
    )
    chosen_ids = pad_sequence(
        chosen_ids, batch_first=True, padding_value=tokenizer.pad_token_id
    )
    rejected_ids = pad_sequence(
        rejected_ids, batch_first=True, padding_value=tokenizer.pad_token_id
    )

    return {
        "input_ids": input_ids,
        "labels": labels,
        "attention_mask": attention_mask,
        "prompt_ids": prompt_ids,
        "chosen_ids": chosen_ids,
        "rejected_ids": rejected_ids,
    }


trainer = SFTWithDPOTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset['train'],
    data_collator=sft_dpo_data_collator,
    dpo_beta=0.1,
    dpo_lambda=0.15,   # æ­£åˆ™æƒé‡0.1
    callbacks=[swanlab_callback],
)


trainer.train()


#VAL_FILE = "./val.jsonl"
RESULT_FILE = "./dpo_result.jsonl"
MAX_NEW_TOKENS = 12 
model.eval() # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼

print(f"æ­£åœ¨è¯»å–éªŒè¯é›†: {VAL_FILE}")
data_samples = []
with open(VAL_FILE, "r", encoding="utf-8") as f:
    for line in f:
        if line.strip():
            data_samples.append(json.loads(line))

print(f"å…±åŠ è½½ {len(data_samples)} æ¡éªŒè¯æ•°æ®ã€‚")
def predict(messages, model, tokenizer):
    """
    ç”Ÿæˆå›å¤å¹¶åªè¿”å›ç”Ÿæˆçš„æ–‡æœ¬éƒ¨åˆ†
    """
    # æ„é€  Prompt
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    inputs = tokenizer([text], return_tensors="pt").to(model.device)
   
    with torch.no_grad():
        generated_ids = model.generate(
            inputs.input_ids,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=False,
            temperature=0.1,  # éªŒè¯æ—¶æ¸©åº¦è®¾ä½ä¸€ç‚¹ï¼Œä¿è¯ç»“æœç¡®å®šæ€§
            top_p=0.9
        )

    # è£å‰ªæ‰ Input éƒ¨åˆ†ï¼Œåªå– Output
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)
    ]
 
    
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return response

def check_correctness(response, ground_truth):
    """
    åˆ¤åˆ«é€»è¾‘ï¼šåªè¦ç”Ÿæˆçš„å›ç­”åŒ…å«å®Œæ•´çš„ ground truthï¼Œå³ä¸ºæ­£ç¡®
    """
    # ç®€å•çš„å­—ç¬¦ä¸²åŒ…å«åˆ¤æ–­
    return ground_truth in response


base_correct_count = 0
ft_correct_count = 0
total_count = 0
results_log = []

print("\nğŸš€ å¼€å§‹è‡ªåŠ¨åŒ–è¯„ä¼°...")
pbar = tqdm(data_samples, desc="Evaluating", unit="sample")

for sample in pbar:
    total_count += 1
    
    # æå–è¾“å…¥å’Œæ ‡å‡†ç­”æ¡ˆ
    input_messages = sample["messages"][:-1]
    ground_truth = sample["messages"][-1]["content"]
    ft_response = predict(input_messages, model, tokenizer)
    is_ft_correct = check_correctness(ft_response, ground_truth)
    if is_ft_correct:
        ft_correct_count += 1

    results_log.append({
        "input": input_messages[-1]["content"], 
        "ground_truth": ground_truth,
        "ft_response": ft_response,
        "ft_correct": is_ft_correct
    })

    current_ft_acc = ft_correct_count / total_count
    pbar.set_postfix({ 
        "FT_Acc": f"{current_ft_acc:.2%}"
    })

final_ft_acc = ft_correct_count / total_count

print("\n" + "="*50)
print("æœ€ç»ˆè¯„ä¼°æŠ¥å‘Š")
print("="*50)
print(f"éªŒè¯é›†æ€»æ•°: {total_count}")
print("åˆ¤åˆ«æ ‡å‡†: ç”Ÿæˆå†…å®¹å¿…é¡»åŒ…å« Ground Truth")
print("-" * 30)
print(f"å¾®è°ƒæ¨¡å‹ æ­£ç¡®æ•°: {ft_correct_count}")
print(f"å¾®è°ƒæ¨¡å‹ å‡†ç¡®ç‡: {final_ft_acc:.2%}")
print("="*50)

# ä¿å­˜è¯¦ç»†ç»“æœ
with open(RESULT_FILE, "w", encoding="utf-8") as f:
    json.dump(results_log, f, ensure_ascii=False, indent=2)
print(f"è¯¦ç»†å¯¹æ¯”æ—¥å¿—å·²ä¿å­˜è‡³: {RESULT_FILE}")

swanlab.finish()
