import argparse
import os
import torch
import json
from tqdm import tqdm
from modelscope import AutoTokenizer
from qwen2.modeling_qwen2 import Qwen2ForCausalLM
from qwen2.configuration_qwen2 import Qwen2Config
from peft import PeftModel
from safetensors.torch import load_file
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel, PrefixTuningConfig

def main():
    parser = argparse.ArgumentParser(description="æµ‹è¯•è„šæœ¬")
    
    # æ·»åŠ å‚æ•°
    parser.add_argument("--ft_method", type=str, default="lora", help="å¾®è°ƒæ–¹æ³•")
    parser.add_argument("--file_path", type=str, required=True, help="æƒé‡æ–‡ä»¶è·¯å¾„")

    
    # è§£æå‚æ•°
    args = parser.parse_args()
    method = args.ft_method

    os.environ["CUDA_VISIBLE_DEVICES"] = "0"
    # è·¯å¾„é…ç½®
    BASE_MODEL_PATH = "./qwen/Qwen2___5-1___5B-Instruct/"
    VAL_FILE = "./val.jsonl"
    RESULT_FILE = "result.jsonl"
    MAX_NEW_TOKENS = 12 

    print(f"æ­£åœ¨åŠ è½½æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)

    if method == 'adapter':
        config = Qwen2Config.from_json_file("./qwen/Qwen2___5-1___5B-Instruct/config.json")
        model = Qwen2ForCausalLM(config)
        model_weights = load_file(args.file_path+'/model.safetensors')
        miss = model.load_state_dict(model_weights, strict=False)
        model = model.to(dtype=torch.bfloat16, device="cuda")
    else:
        model = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL_PATH,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True,
            load_in_4bit=True,
        )
        model = PeftModel.from_pretrained(model, args.file_path)
     
    model.eval()
    print(f"æ­£åœ¨è¯»å–éªŒè¯é›†: {VAL_FILE}")
    data_samples = []
    with open(VAL_FILE, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                data_samples.append(json.loads(line))

    print(f"å…±åŠ è½½ {len(data_samples)} æ¡éªŒè¯æ•°æ®ã€‚")
    
    def predict(messages, model, tokenizer):
        """
        ç”Ÿæˆå›å¤å¹¶åªè¿”å›ç”Ÿæˆçš„æ–‡æœ¬éƒ¨åˆ†
        """
        # æ„é€  Prompt
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        inputs = tokenizer([text], return_tensors="pt").to(model.device)

        with torch.no_grad():
            generated_ids = model.generate(
                **inputs,
                max_new_tokens=MAX_NEW_TOKENS,
                temperature=0.1,  # éªŒè¯æ—¶æ¸©åº¦è®¾ä½ä¸€ç‚¹ï¼Œä¿è¯ç»“æœç¡®å®šæ€§
                top_p=0.9
            )

        # è£å‰ªæ‰ Input éƒ¨åˆ†ï¼Œåªå– Output
        generated_ids = [
            output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)
        ]
        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        return response

    def check_correctness(response, ground_truth):
        """
        åˆ¤åˆ«é€»è¾‘ï¼šåªè¦ç”Ÿæˆçš„å›ç­”åŒ…å«å®Œæ•´çš„ ground truthï¼Œå³ä¸ºæ­£ç¡®
        """
        return ground_truth in response

    base_correct_count = 0
    ft_correct_count = 0
    total_count = 0
    results_log = []

    print("\nğŸš€ å¼€å§‹è‡ªåŠ¨åŒ–è¯„ä¼°...")
    pbar = tqdm(data_samples, desc="Evaluating", unit="sample")

    for sample in pbar:
        total_count += 1

        input_messages = sample["messages"][:-1]
        ground_truth = sample["messages"][-1]["content"]

        ft_response = predict(input_messages, model, tokenizer)
        print(ft_response)
        is_ft_correct = check_correctness(ft_response, ground_truth)
        if is_ft_correct:
            ft_correct_count += 1

        results_log.append({
            "input": input_messages[-1]["content"], # è®°å½•æœ€åä¸€ä¸ªé—®é¢˜
            "ground_truth": ground_truth,
            "ft_response": ft_response,
            "ft_correct": is_ft_correct
        })

        current_ft_acc = ft_correct_count / total_count
        pbar.set_postfix({ 
            "FT_Acc": f"{current_ft_acc:.2%}"
        })

    final_ft_acc = ft_correct_count / total_count

    print("\n" + "="*50)
    print("æœ€ç»ˆè¯„ä¼°æŠ¥å‘Š")
    print("="*50)
    print(f"éªŒè¯é›†æ€»æ•°: {total_count}")
    print("åˆ¤åˆ«æ ‡å‡†: ç”Ÿæˆå†…å®¹å¿…é¡»åŒ…å« Ground Truth")
    print("-" * 30)
    print(f"å¾®è°ƒæ¨¡å‹ æ­£ç¡®æ•°: {ft_correct_count}")
    print(f"å¾®è°ƒæ¨¡å‹ å‡†ç¡®ç‡: {final_ft_acc:.2%}")
    print("="*50)

    # ä¿å­˜è¯¦ç»†ç»“æœ
    with open(RESULT_FILE, "w", encoding="utf-8") as f:
        json.dump(results_log, f, ensure_ascii=False, indent=2)
    print(f"è¯¦ç»†å¯¹æ¯”æ—¥å¿—å·²ä¿å­˜è‡³: {RESULT_FILE}")

if __name__ == "__main__":
    main()